# Vachanamrut AI Backend

A powerful AI-driven backend for the Vachanamrut Chatbot, designed to help users search, query, and explore the Vachanamrut scripture using natural language.

## ğŸ“Œ Overview

This project provides a robust API built with **FastAPI** that integrates **Retrieval-Augmented Generation (RAG)** to provide accurate, context-aware answers from the Vachanamrut.

It features a unique hybrid LLM architecture that utilizes **Groq (Llama 3)** for ultra-fast primary inference and **Google Gemini** as a reliable fallback, determining the best response strategy dynamically.

## ğŸš€ Key Features

- **âš¡ High-Speed Inference**: Powered by **Groq** using the Llama 3 model family.
- **ğŸ”„ Robust Fallback System**: Automatically switches to **Google Gemini (Flash 1.5)** if Groq limits are reached or errors occur.
- **ğŸ§  Semantic Search**: Uses **ChromaDB** and **Sentence-Transformers** (`all-MiniLM-L6-v2`) to understand the meaning behind user queries, not just keyword matching.
- **ğŸŒŠ Streaming Responses**: Supports Server-Sent Events (SSE) for real-time, typewriter-style AI responses.
- **ğŸ“‚ Modular Architecture**: Clean separation of concerns with a dedicated Agent Orchestrator, Service layer, and Data management.
- **ğŸ” Granular Filtering**: Allows users to filter searches by specific chapters, sections, or Vachanamrut numbers.

## ğŸ› ï¸ Tech Stack

- **Framework**: [FastAPI](https://fastapi.tiangolo.com/)
- **Vector Database**: [ChromaDB](https://www.trychroma.com/)
- **LLM Providers**: 
  - [Groq](https://groq.com/) (Llama-3.3-70b-versatile)
  - [Google Gemini](https://ai.google.dev/) (Gemini-1.5-flash)
- **Embeddings**: [SentenceTransformers](https://www.sbert.net/)
- **Language**: Python 3.9+

## ğŸ“¦ Installation

### Prerequisites
- Python 3.9 or higher installed.
- API Keys for **Groq** and **Google Gemini**.

### 1. Clone the Repository
```bash
git clone https://github.com/Krut369/vachnamrut_backend.git
cd vachnamrut_backend
```

### 2. Create a Virtual Environment
It is recommended to use a virtual environment to manage dependencies.
```bash
python -m venv venv
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
# Note: Ensure you also install google-generativeai if not in requirements
pip install google-generativeai
```

## ğŸ”‘ Configuration

Create a `.env` file in the root directory of the project and add your API keys. You can provide multiple keys separated by commas for easy rotation/fallback.

```env
# Primary Provider (Groq)
GROQ_API_KEY=gsk_your_key_1,gsk_your_key_2

# Fallback Provider (Gemini)
GEMINI_API_KEYS=AIza_your_key_1,AIza_your_key_2
```

## ğŸƒâ€â™‚ï¸ Running the Server

Start the development server with hot-reload enabled:

```bash
python main.py
```
*Alternatively, you can run directly with Uvicorn:*
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The API will be available at: **http://localhost:8000**

## ğŸ“š API Documentation

Interactive API documentation is automatically generated by FastAPI:

- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs) - Test endpoints directly in your browser.
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc) - Alternative documentation format.

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| `GET` | `/health` | Check if API and all modules (VectorDB, Agent) are running. |
| `GET` | `/vachanamrut` | Fetch specific Vachanamrut text by Chapter, Section, and Number. |
| `POST` | `/ask` | **Streaming Endpoint**. Sends a user query and returns an AI-generated response chunk-by-chunk. |

## ğŸ“‚ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agent/            # ğŸ§  AI Orchestration logic, Prompts, and Steps
â”‚   â”œâ”€â”€ api/              # ğŸŒ API Routes (if separated)
â”‚   â”œâ”€â”€ core/             # âš™ï¸ Configuration & Settings
â”‚   â”œâ”€â”€ models/           # ğŸ“¦ Pydantic Data Models
â”‚   â”œâ”€â”€ services/         # ğŸ”§ Business Logic (VectorService, LLMService, Librarian)
â”‚   â””â”€â”€ main.py           # ğŸš€ Application Entry Point
â”œâ”€â”€ data/                 # ğŸ’½ Local VectorDB Storage & JSON Data
â”œâ”€â”€ .env                  # ğŸ” Environment Variables (GitIgnored)
â””â”€â”€ requirements.txt      # ğŸ Project Dependencies
```
